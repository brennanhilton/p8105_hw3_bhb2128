---
title: "Homework_3"
author: "Brennan Baker"
date: "October 9, 2018"
output:
  github_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Problem 1

Load packages
```{r packages}
library(tidyverse)
library(p8105.datasets)
```



*Tidy the data*


```{r tidy brfss data}
brfss = brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(topic == "Overall Health") %>% 
  separate(locationdesc, into = c("state", "county"), sep = "- ") %>% 
  select(year, state, county, response, data_value) %>% 
  mutate(response = tolower(response)) %>% 
  mutate(response = ordered(response, levels = c("excellent", "very good", "good", "fair", "poor")))
```

I cleaned the data by: cleaning the names; keeping only the rows where the topic is overall health; separating "locationdesc" into state and county columns; and excluding unnecessary variables. I converted response to a factor varaible and arranged it with "Excellent"" on top.

*In 2002, which states were observed at 7 locations?*

```{r states with 7 locations}
brfss %>% 
  filter(year == "2002") %>% 
  distinct(state, county, .keep_all = TRUE) %>% 
  count(state) %>% 
  filter(n == "7") %>% 
  nrow()
```

There were 3 states observed at 7 locations. 

*Make a “spaghetti plot” that shows the number of locations in each state from 2002 to 2010*

```{r spaghetti plot}
brfss %>%
  distinct(state, county, .keep_all = TRUE) %>%
  group_by(year) %>% 
  count(state) %>% 
    ggplot(aes(x = year, y = n, color = state)) +
  geom_line() +
  labs(
    title = "Distinct locations in each state",
    x = "Year",
    y = "Number of locations"
  ) + 
  theme_bw()
```

*Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.*


```{r proportion of excellent table}
brfss %>%
  filter(year %in% c("2002", "2006", "2010"), # filter years, state, and excellent responses
         str_detect(state, "NY"),
         response == "excellent",
         !is.na(data_value)) %>% 
  group_by(year) %>% 
  summarize(mean = mean(data_value), std_dev = sd(data_value)) %>% 
  knitr::kable(digits = 4)
```


*For each year and state, compute the average proportion in each response category (taking the average across locations in a state). Make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time.*

```{r five panel plot}
brfss %>%
  group_by(year, state, response) %>%
  filter(!is.na(data_value)) %>% 
  summarize(mean = mean(data_value)) %>% 
  ggplot(aes(x = year, y = mean, color = response)) +
  geom_point() +
  facet_grid(~response) +
  labs(
    title = "Proportion of each response",
    x = "Mean proportion",
    y = "Year"
  ) + 
  theme_bw() +
  viridis::scale_color_viridis(discrete = TRUE) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
  
```

# Problem 2


Instacart is an online grocery shopping service. This instacart dataset contains `r nrow(instacart)` rows and `r ncol(instacart)` columns. The dataset contains one order from each of `r instacart %>% distinct(order_id) %>% nrow()` distinct users. Each row is a distinct product from one of these orders. The data set includes more information for each product, including the department and aisle it belongs to.  As an example, the table below shows the products from just one order (order_id = 1). From the table we can see that the first product is "Bulgarian Yogurt", which belongs to the "yogurt" aisle in the "dairy eggs" department.

```{r echo = FALSE}
instacart %>% 
  filter(order_id == "1") %>% 
  knitr::kable()
```


*How many aisles are there, and which aisles are the most items ordered from?*
```{r number of aisles}
instacart %>% distinct(aisle, .keep_all = TRUE) %>% nrow()
```

There are `r instacart %>% distinct(aisle, .keep_all = TRUE) %>% nrow()` distinct aisles.

The table below shows the top 5 aisles that contain the most ordered items. 

```{r aisles with most items ordered}
instacart %>% distinct(aisle, product_name, .keep_all = TRUE) %>%
  count(aisle) %>% 
  arrange(desc(n)) %>% 
  top_n(5) %>% 
  knitr::kable()
```


*Make a plot that shows the number of items ordered in each aisle. Order aisles sensibly, and organize your plot so others can read it.*


```{r items ordered in each aisle plot}
instacart %>% distinct(aisle, product_name, .keep_all = TRUE) %>%
  group_by(aisle, department) %>% 
  filter(!aisle %in% c("other", "missing")) %>% # removed other and missing because those aisles are un informative
  summarise(number_items = n()) %>%
  ggplot(aes(x = department, y = number_items)) +
  geom_boxplot() +
   labs(
    title = "Number of items ordered in each aisle by department",
    x = "Department",
    y = "Number of items per aisle"
  ) + 
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

*Make a table showing the most popular item aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”*

```{r popular items table}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle, product_name) %>%
  summarize(number = n()) %>% 
  arrange(desc(number)) %>% 
  top_n(1) %>% 
  knitr::kable()
```


*Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).*
tried this code in the below chunck it didnt work
mutate(order_dow = recode(order_dow, 0 = "Sunday", 1 = "Monday", 2 = "Tuesday", 3 = "Wednesday", 4 = "Thursday", 5 = "Friday", 6 = "Saturday"))

```{r hour of day table}
instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  spread(key = order_dow, value = mean_hour) %>% 
  knitr::kable()
```


# Problem 3

The National Oceanic and Atmospheric Association (NOAA) provides weather data. This dataset contains `r nrow(ny_noaa)` rows and `r ncol(ny_noaa)` columns. The data are for five variables measured from all weahter stations daily in New York state from January 1, 1981 through December 31, 2010. The variables are precipiation, snowfall, snow depth, maximum temperature, and minimum temperature. There is a lot of missing data, which is an issue for calculating descriptive statistics in r. Thus, missing values will often need to be filtered out.

*Do some data cleaning. Create separate variables for year, month, and day. Ensure observations for temperature, precipitation, and snowfall are given in reasonable units. For snowfall, what are the most commonly observed values? Why?*

First I need to load this package for working with dates.
```{r load package}
library(lubridate) # need this package below for separating the date into multiple columns
```

This code tidys the data by making new columns for year, month, and day, and converting any variable expressed in tenths of a unit to whole units. 
```{r tidy noaa data}
noaa_data = ny_noaa %>%
  mutate_at(vars(date), funs(year, month, day)) %>% # need the lubridate package for this line
  mutate(prcp = prcp/10, tmax = as.numeric(tmax)/10, tmin = as.numeric(tmin)/10) # convert from tenths of mm and tench of degrees to whole units
```

*For snowfall, what are the most commonly observed values? Why?*
```{r most common snowfall}
noaa_data %>% 
  count(snow) %>% 
  arrange(desc(n))
```
The most commonly observed value is 0 because on most days it does not snow.

*Make a two-panel plot showing the average max temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?*

```{r jan and july temp plot}
noaa_data %>% 
  filter(month %in% c(1, 7)) %>% # keeps only data for months of Jan and July
  mutate(month = as.factor(month)) %>% # needed to convert to factor for the next line to work
  mutate(month = recode(month, "1" = "January", "7" = "July")) %>%
  group_by(month, id, year) %>% 
  filter(!is.na(tmax)) %>%
  summarize(mean_tmax = mean(tmax)) %>%
  ggplot(aes(x = year, y = mean_tmax, color = id)) +
  geom_line() +
  facet_grid(~month) +
  labs(
    title = "Maximum temperature plot",
    x = "Year",
    y = "Mean maximum temperature"
  ) + 
  theme_bw() +
  theme(legend.position = "none") +
  theme(panel.spacing = unit(2, "lines")) # adds space between faceted plots
  
```


*Make a two-panel plot showing (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option); and (ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.*

```{r temp and snow plots plots, fig.height = 10}
library(hexbin) # geom_hex did not work until I loaded this package
library(patchwork) # this package lets me make the two panel plot with the last line of code in this chunk
tmax_tmin = noaa_data %>% 
  filter(!is.na(tmax),
         !is.na(tmin)) %>%
  ggplot(aes(x = tmin, y = tmax)) +
  geom_hex() +
  labs(
    title = "Daily temperatures at all NY state weather stations",
    x = "Minimum daily temperature (C)",
    y = "Maxiumum daily temperature (C)"
  ) + 
  theme_bw() +
  guides(fill=guide_legend(title="Count from\nJanuary 1, 1981\nthrough\nDecember 31, 2010"))
 

snow_plot = noaa_data %>% 
  filter(snow > 0,
         snow < 100) %>% 
  mutate(year = as.factor(year)) %>% 
  ggplot(aes(x = year, y = snow)) +
  geom_boxplot() +
  labs(
    title = "Snowfall distribution by year",
    x = "Year",
    y = "Snowfall (mm)"
  ) + 
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

tmax_tmin / snow_plot

```

